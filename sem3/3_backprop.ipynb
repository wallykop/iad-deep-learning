{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "3_backprop.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhFUZE3Ksss7",
        "colab_type": "text"
      },
      "source": [
        "# Современные методы машинного обучения, ИАД\n",
        "\n",
        "## Семинар 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdnWfp2Esss_",
        "colab_type": "text"
      },
      "source": [
        "## Градиентный спуск: цепное правило"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_37a3HsstA",
        "colab_type": "text"
      },
      "source": [
        "Из курса матанализа мы умеем дифферинциировать простые функции, например:\n",
        "\n",
        "$\\frac{dx^2}{dx} = 2x$  \n",
        "\n",
        "$\\frac{de^x}{dx} = e^ x$   \n",
        "\n",
        "$\\frac{dln(x)}{dx} = \\frac{1}{x}$\n",
        "\n",
        "Для использования цепного правила в градиентном спуске нам понадобится дифференциировать сложные функции.\n",
        "\n",
        "Сложная функция — это функция от функции.<br>Если u — функция от x, то есть u=u(x),  а f — функция от u:  f=f(u), то функция y=f(u) — сложная.\n",
        "\n",
        "Возьмем сложную функцию:<br>\n",
        "$z_1 = z_1(x_1, x_2)$<br>\n",
        "$z_2 = z_2(x_1, x_2)$<br>\n",
        "$p = p(z_1, z_2)$<br>\n",
        "\n",
        "где $z_1,z_2,p$ дифференциируемы\n",
        "\n",
        "\n",
        "Применим цепное правило:<br>\n",
        "### $\\frac{\\partial p}{\\partial x_1} = \\frac{\\partial p}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_1} + \\frac{\\partial p}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_1}$\n",
        "### $\\frac{\\partial p}{\\partial x_2} = \\frac{\\partial p}{\\partial z_1} \\frac{\\partial z_1}{\\partial x_2} + \\frac{\\partial p}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_2}$\n",
        "\n",
        "<br>\n",
        "пример для $h(x) = f(x)g(x)$: \n",
        "### $\\frac{\\partial h}{\\partial x} = \\frac{\\partial h}{\\partial f} \\frac{\\partial f}{\\partial x} + \\frac{\\partial h}{\\partial g} \\frac{\\partial g}{\\partial x} = g \\frac{\\partial f}{\\partial x} + f \\frac{\\partial g}{\\partial x}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuLnyAaGsstA",
        "colab_type": "text"
      },
      "source": [
        "Построим граф вычисления для нашей композиции:\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic1.png?raw=1\" alt=\"Drawing\" style=\"width: 400px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHPErPGusstH",
        "colab_type": "text"
      },
      "source": [
        "Построим из него граф производных где каждому ребру будет прописана производная начала по концу:\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic2.png?raw=1\" alt=\"Drawing\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDB-UMTesstH",
        "colab_type": "text"
      },
      "source": [
        "Можно догадаться как работает цепное правило\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic3.png?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kc44KjysstI",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "Добавим еще один скрытый слой:\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic4.png?raw=1\" alt=\"Drawing\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZKeATmQsstJ",
        "colab_type": "text"
      },
      "source": [
        "Применим цепное правило несколько раз:\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic5.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlxeZF5BsstJ",
        "colab_type": "text"
      },
      "source": [
        "Рассмотрим что из себя представляет каждое слагаемое\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic6.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic7.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic8.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic9.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZR0xPFVsstK",
        "colab_type": "text"
      },
      "source": [
        "### Алгоритм вычисления производной в графе"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrI3-FotsstL",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic10.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aucoiGYRsstL",
        "colab_type": "text"
      },
      "source": [
        "Рассмотрим простую нейросеть:\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic11.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/wallykop/iad-deep-learning/blob/master/sem3/pic12.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLPK1agwsstM",
        "colab_type": "text"
      },
      "source": [
        "<font size=\"5\">\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "<br>\n",
        "$L = 0.5*(y - z)^2$\n",
        "<br>\n",
        "$\\frac{\\partial L}{\\partial z} = ? $\n",
        "<br>\n",
        "$\\frac{\\partial L}{\\partial \\alpha} = ? $\n",
        "<br>\n",
        "$\\frac{\\partial L}{\\partial \\beta} = ? $\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEu_qqSLsstN",
        "colab_type": "text"
      },
      "source": [
        "** Упражнение 1 **\n",
        "\n",
        "Сделайте три шага градиентного спуска и заполните таблицу при:\n",
        "$ \\alpha_0 = 0.5 $<br> $ \\beta_0 = 1 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuTMrzYsstP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "| $x_1$ | $x_2$ | y | s | z | L | $\\frac{\\partial L}{ \\partial z}$ | $\\frac{\\partial \\sigma}{ \\partial s}$ | $\\frac{\\partial L}{ \\partial \\alpha}$ | $\\frac{\\partial L}{ \\partial \\beta}$ | $\\alpha$ | $\\beta$ |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  |\n",
        "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  |\n",
        "| 1 | 2 | 3 |  |  |  |  |  |  |  |  |  | |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5wxYpgnsstQ",
        "colab_type": "text"
      },
      "source": [
        "### О задании\n",
        "\n",
        "Задание посвящено реализации различных слоёв нейронной сети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU96FfCEsstR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOYkjpoLsstU",
        "colab_type": "text"
      },
      "source": [
        "## Часть 1. Реализация слоёв графа вычислений\n",
        "\n",
        "В этом задании мы реализуем граф вычислений для задачи распознавания изображений рукописных цифр на примере датасета [MNIST](http://yann.lecun.com/exdb/mnist/) — в частности, эта часть посвящена реализации всех требующихся для построения графа слоёв.\n",
        "\n",
        "Указанная задача является задачей классификации на $K = 10$ классов, поэтому будем строить граф вычислений, выходной слой которого будет содержать 10 нейронов, $k$-ый из которых вычисляет оценку принадлежности объекта $k$-ому классу. В качестве функционала качества в данной задаче будем использовать **кросс-энтропию**:\n",
        "\n",
        "$$Q(a, X) = -\\frac{1}{l}\\sum_{i=1}^l \\sum_{k=1}^K [y_i = k] \\log a_k(x_i),$$\n",
        "где\n",
        "\n",
        "$X = \\{ (x_i, y_i)\\}_{i=1}^l, \\, y_i \\in \\{1, \\dots, K\\},$ — обучающая выборка,\n",
        "\n",
        "$a(x) = (a_k(x))_{k=1}^K \\in \\mathbb{R}^K$ — прогноз графа вычислений для объекта $x$, состоящий из выходов $K$ нейронов выходного слоя (т.е. $a_k(x)$ — оценка принадлежности объекта $x$ классу $k$, построенная при помощи заданного графа вычислений).\n",
        "\n",
        "Нейрнонные сети обучаются с использованием стохастических методов оптимизации, однако для ускорения обучения и большей стабильности за один проход параметры оптимизируются по батчу — набору из нескольких тренировочных примеров, так же batch_size является дополнительной размерностью для входящих в слой тензоров.\n",
        "\n",
        "Для начала определим класс Layer, реализующий тождественный слой, который будет являться базовым классом для всех последующих."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q44vXoisstV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "\n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "\n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "\n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Here you can initialize layer parameters (if any) and auxiliary stuff.\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError(\"Not implemented in interface\")\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, ...], returns output data [batch, ...]\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError(\"Not implemented in interface\")\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "\n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "\n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "\n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError(\"Not implemented in interface\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-dI9v7_sstX",
        "colab_type": "text"
      },
      "source": [
        "**Задание 1**\n",
        "\n",
        "Используя приведенные прототипы, реализуйте слой, применяющий функцию активации ReLU (Rectified Linear Unit) поэлементно к каждому из входов слоя:\n",
        "$$\\text{ReLU}(z) = \\max (0, z)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtAsTIw5sstY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
        "        This layer does not have any parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform ReLU transformation\n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, input_units]\n",
        "        \"\"\"\n",
        "        return NotImplementedError(\"Not implemented in interface\")\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Compute gradient of loss w.r.t. ReLU input\n",
        "        \"\"\"\n",
        "        return NotImplementedError(\"Not implemented in interface\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKnSOBHbssta",
        "colab_type": "text"
      },
      "source": [
        "**Задание 2**\n",
        "\n",
        "Используя указанные прототипы, реализуйте полносвязный слой, выход которого вычисляется следующим образом (подробнее в соответствующей [лекции](https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture11-dl.pdf)):\n",
        "\n",
        "$$f(v; W, b)= Wv + b, $$\n",
        "\n",
        "где\n",
        "* v — выход предыдущего слоя (вектор размера num_inputs);\n",
        "* W — матрица весов [num_inputs, num_outputs];\n",
        "* b — столбец свободных членов (вектор размера num_outputs).\n",
        "\n",
        "При каждом вызове backward() необходимо расчитать градиенты по выходу, используя chain-rule, и сделать один шаг градиентного спуска."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKCaalOsstd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense(Layer):\n",
        "\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = Wx + b\n",
        "\n",
        "        W: matrix of shape [num_inputs, num_outputs]\n",
        "        b: vector of shape [num_outputs]\n",
        "        \"\"\"\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "\n",
        "        return NotImplementedError(\"Not implemented in interface\")\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "\n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        return NotImplementedError(\"Not implemented in interface\")\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Computes d f / d x = d f / d dense * d dense / d x,\n",
        "        where d dense/ d x = weights transposed, and performs\n",
        "        one step of gradient descent on W and b.\n",
        "\n",
        "        input shape: [batch, input_units]\n",
        "        grad_output: [batch, output units]\n",
        "\n",
        "        Returns: grad_input, gradient of output w.r.t input\n",
        "        \"\"\"\n",
        "        return NotImplementedError(\"Not implemented in interface\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21369ftasstg",
        "colab_type": "text"
      },
      "source": [
        "**Задание 3**\n",
        "\n",
        "Как было сказано ранее, в качестве функционала качества в данной задаче мы будем использовать кросс-энтропию. Используя прототипы ниже, реализуйте вычисление данного функционала и его градиента по выходам графа вычислений.\n",
        "\n",
        "Кросс-энтропия предполагает, что модель для каждого объекта выдает вероятности принадлежности к каждому из $K$ классов, т.е. что для одного объекта все $K$ вероятностей неотрицательны и суммируются в 1. В нашем же случае в построении графа участвуют только полносвязный и ReLU слои, а потому выходы графа не являются вероятностями — как правило, в этом случае прогноз $a(x)$ модели нормируется при помощи функции softmax следующим образом:\n",
        "\n",
        "$$\\text{softmax}(a_k(x)) = \\frac{\\exp(a_k(x))}{\\sum_{k=1}^K \\exp(a_k(x))}.$$\n",
        "\n",
        "При реализации указанных функций предполагается, что переданные в качестве параметров оценки принадлежности объектов классам не являются нормированными (их еще называют логитами), но при вычислении указанных величин используйте указанное выше преобразование для приведения этих оценок к корректному виду."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wABqGVi5sstg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_crossentropy_with_logits(logits, y_true):\n",
        "    \"\"\"\n",
        "    Compute crossentropy from logits and ids of correct answers\n",
        "    logits shape: [batch_size, num_classes]\n",
        "    y_true: [batch_size]\n",
        "    output is a number\n",
        "    \"\"\"\n",
        "    return NotImplementedError(\"Not implemented in interface\")\n",
        "\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits, y_true):\n",
        "    \"\"\"\n",
        "    Compute crossentropy gradient from logits and ids of correct answers\n",
        "    logits shape: [batch_size, num_classes]\n",
        "    y_true: [batch_size]\n",
        "    \"\"\"\n",
        "    return NotImplementedError(\"Not implemented in interface\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYmuCMp1sstq",
        "colab_type": "text"
      },
      "source": [
        "## Часть 2. Реализация и применение графа вычислений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5EH-QxQsstq",
        "colab_type": "text"
      },
      "source": [
        "В этой части мы научимся объединять слои в единый граф вычислений, а также использовать его для прямого прохода (вычисления прогнозов на объектах) и обратного прохода (обновление обучаемых параметров графа), после чего у нас появится возможность обучить граф. Для простоты реализации будем считать, что в нашем случае граф вычислений задается как список (python list) слоёв из числа реализованных ранее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyCgvVfdsstr",
        "colab_type": "text"
      },
      "source": [
        "Ниже приведен код для скачивания датасета MNIST с официального сайта. Датасет делится на тренировочную и тестовую части. Тренировочная дополнительно разбивается на тренировочную и валидационную."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CN5y4cfssts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import gzip\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def load_mnist(flatten=False):\n",
        "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
        "    # We first define a download function, supporting both Python 2 and 3.\n",
        "\n",
        "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
        "        print(\"Downloading %s\" % filename)\n",
        "        urlretrieve(source + filename, filename)\n",
        "\n",
        "    # We then define functions for loading MNIST images and labels.\n",
        "    # For convenience, they also download the requested files if needed.\n",
        "\n",
        "    def load_mnist_images(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the inputs in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
        "        # following the shape convention: (examples, channels, rows, columns)\n",
        "        data = data.reshape(-1, 1, 28, 28)\n",
        "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
        "        # (Actually to range [0, 255/256], for compatibility to the version\n",
        "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
        "        return data / np.float32(256)\n",
        "\n",
        "    def load_mnist_labels(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the labels in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        # The labels are vectors of integers now, that's exactly what we want.\n",
        "        return data\n",
        "\n",
        "    # We can now download and read the training and test set images and labels.\n",
        "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # We reserve the last 10000 training examples for validation.\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "    \n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "    # We just return all the arrays in order, as expected in main().\n",
        "    # (It doesn't matter how we do this as long as we can read them again.)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2v8vBVBsstu",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на несколько объектов из этого датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN1300ZHsstx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
        "\n",
        "plt.figure(figsize=[6, 6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28, 28]),cmap='gray');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMugx8vWsstz",
        "colab_type": "text"
      },
      "source": [
        "**Задание 4**\n",
        "\n",
        "Используя прототип ниже, реализуйте прямой и обратный проход по графу вычислений и функцию для получения предсказаний метки класса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG45HxX4sstz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        layers — list of Layer objects\n",
        "        \"\"\"\n",
        "        \n",
        "        self.layers = layers\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute activations of all network layers by applying them sequentially.\n",
        "        Return a list of activations for each layer. \n",
        "        Make sure last activation corresponds to network logits.\n",
        "        \"\"\"\n",
        "        \n",
        "        activations = []\n",
        "        input = X\n",
        "\n",
        "        raise NotImplementedError(\"Implement me plz ;(\")\n",
        "\n",
        "        assert len(activations) == len(self.layers)\n",
        "        return activations\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use network to predict the most likely class for each sample.\n",
        "        \"\"\"\n",
        "        \n",
        "        raise NotImplementedError(\"Implement me plz ;(\")\n",
        "        \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Train your network on a given batch of X and y.\n",
        "        You first need to run forward to get all layer activations.\n",
        "        Then you can run layer.backward going from last to first layer.\n",
        "\n",
        "        After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the layer activations\n",
        "        layer_activations = self.forward(X)\n",
        "        layer_inputs = [X] + layer_activations  # layer_input[i] is an input for network[i]\n",
        "        logits = layer_activations[-1]\n",
        "\n",
        "        # Compute the loss and the initial gradient\n",
        "        loss = softmax_crossentropy_with_logits(logits, y)\n",
        "        loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
        "\n",
        "        # propagate gradients through network layers using .backward\n",
        "        # hint: start from last layer and move to earlier layers\n",
        "        raise NotImplementedError(\"Implement me plz ;(\")\n",
        "\n",
        "        return np.mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uhf6mX8sst1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = []\n",
        "hidden_layers_size = 40\n",
        "layers.append(Dense(X_train.shape[1], hidden_layers_size))\n",
        "layers.append(ReLU())\n",
        "layers.append(Dense(hidden_layers_size, hidden_layers_size))\n",
        "layers.append(ReLU())\n",
        "layers.append(Dense(hidden_layers_size, 10))\n",
        "\n",
        "model = NeuralNetwork(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZs1DFq-sst4",
        "colab_type": "text"
      },
      "source": [
        "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве должна превысить 97%. \n",
        "\n",
        "Ниже определена функции для итерации по батчам, принимающая на вход картинки, метки классов, а также размер батча и флаг, отвечающий за перемешивание примеров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmbxApW0sst4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, seed=1234):\n",
        "    assert len(inputs) == len(targets)\n",
        "    \n",
        "    indices = np.arange(len(inputs)).astype(np.int32)\n",
        "    if shuffle:\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(indices)\n",
        "    \n",
        "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        batch = indices[start_idx:start_idx + batchsize]\n",
        "        \n",
        "        yield inputs[batch], targets[batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgdsxqQjsst9",
        "colab_type": "text"
      },
      "source": [
        "Ниже приведены функции для обучения модели и отслеживания значения loss на тренироворочной части и на валидации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6mGjtQrsst_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQoUheBvssuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(15):\n",
        "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        "        model.backward(x_batch, y_batch)\n",
        "    \n",
        "    train_log.append(np.mean(model.predict(X_train) == y_train))\n",
        "    val_log.append(np.mean(model.predict(X_val) == y_val))\n",
        "    \n",
        "    clear_output()\n",
        "    print(\"Epoch\", epoch)\n",
        "    print(\"Train accuracy:\", train_log[-1])\n",
        "    print(\"Val accuracy:\", val_log[-1])\n",
        "    plt.plot(train_log, label='train accuracy')\n",
        "    plt.plot(val_log, label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQSNfgussuH",
        "colab_type": "text"
      },
      "source": [
        "## Часть 3. Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18GqfsPyssuJ",
        "colab_type": "text"
      },
      "source": [
        "В этой части мы увидим как с помощью фреймворков с автоматическим дифференцированием, создание нейронных сетей занимает намного меньше времени."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9sBLLz0ssuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsAMrLmNssuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJYqJKszssuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add() # создайте аналогичную сеть как вы делали выше\n",
        "\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=SGD(lr=0.01),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=15,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}